{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Count common words, overall word count and question length\n",
    "def common_words(x):\n",
    "    q1, q2 = x\n",
    "    return len(set(str(q1).lower().split()) & set(str(q2).lower().split()))\n",
    "\n",
    "def words_count(question):\n",
    "    return len(str(question).split())\n",
    "\n",
    "def length(question):\n",
    "    return len(str(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features Set 1 (FS1)\n",
    "#Count common words, overall word count and question length for both Train and Test Data\n",
    "traindata['q1_words_num'] = traindata['q1_cleaned'].map(words_count)\n",
    "traindata['q2_words_num'] = traindata['q2_cleaned'].map(words_count)\n",
    "traindata['q1_length'] = traindata['q1_cleaned'].map(length)\n",
    "traindata['q2_length'] = traindata['q2_cleaned'].map(length)\n",
    "traindata['common_words'] = traindata[['q1_cleaned', 'q2_cleaned']].apply(common_words, axis=1)\n",
    "\n",
    "testdata['q1_words_num'] = testdata['q1_cleaned'].map(words_count)\n",
    "testdata['q2_words_num'] = testdata['q2_cleaned'].map(words_count)\n",
    "testdata['q1_length'] = testdata['q1_cleaned'].map(length)\n",
    "testdata['q2_length'] = testdata['q2_cleaned'].map(length)\n",
    "testdata['common_words'] = testdata[['q1_cleaned', 'q2_cleaned']].apply(common_words, axis=1)\n",
    "\n",
    "#Count difference in question length for Train and Test Data\n",
    "traindata['diff_length'] = traindata.q1_length - traindata.q2_length\n",
    "testdata['diff_length'] = testdata.q1_length - testdata.q2_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features Set 2 (FS2)\n",
    "#Create Fuzzy features for the traindata\n",
    "from fuzzywuzzy import fuzz\n",
    "traindata['fuzz_qratio'] = traindata.apply(lambda x: fuzz.QRatio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1)\n",
    "traindata['fuzz_WRatio'] = traindata.apply(lambda x: fuzz.WRatio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1) \n",
    "traindata['fuzz_partial_ratio'] = traindata.apply(lambda x: fuzz.partial_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1) \n",
    "traindata['fuzz_partial_token_set_ratio'] = traindata.apply(lambda x: fuzz.partial_token_set_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1)\n",
    "traindata['fuzz_partial_token_sort_ratio'] = traindata.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1) \n",
    "traindata['fuzz_token_set_ratio'] = traindata.apply(lambda x: fuzz.token_set_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1)\n",
    "traindata['fuzz_token_sort_ratio'] = traindata.apply(lambda x: fuzz.token_sort_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1)\n",
    "\n",
    "#Create Fuzzy features for the traindatatestdata\n",
    "testdata['fuzz_qratio'] = testdata.apply(lambda x: fuzz.QRatio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1)\n",
    "testdata['fuzz_WRatio'] = testdata.apply(lambda x: fuzz.WRatio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1) \n",
    "testdata['fuzz_partial_ratio'] = testdata.apply(lambda x: fuzz.partial_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1) \n",
    "testdata['fuzz_partial_token_set_ratio'] = testdata.apply(lambda x: fuzz.partial_token_set_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1)\n",
    "testdata['fuzz_partial_token_sort_ratio'] = testdata.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1) \n",
    "testdata['fuzz_token_set_ratio'] = testdata.apply(lambda x: fuzz.token_set_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1)\n",
    "testdata['fuzz_token_sort_ratio'] = testdata.apply(lambda x: fuzz.token_sort_ratio(str(x['q1_cleaned']), str(x['q2_cleaned'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word2vec features \n",
    "\n",
    "import gensim\n",
    "#Merging all sencences\n",
    "sentences = traindata['q1_cleaned'].values.tolist() +  traindata['q2_cleaned'].values.tolist()\n",
    "\n",
    "#Creating a model \n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2vec(words): #takes array of cleaned tokens \n",
    "#words = str(s).lower().decode('utf-8')\n",
    "#words = word_tokenize(words)\n",
    "#words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        M.append(model[w])\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create word2vec vectors for both Train and Test Data based on cleaned questions\n",
    "traindata['q1_vec'] = traindata['q1_cleaned'].apply(sent2vec)\n",
    "traindata['q2_vec'] = traindata['q2_cleaned'].apply(sent2vec)\n",
    "\n",
    "testdata['q1_vec'] = testdata['q1_cleaned'].apply(sent2vec)\n",
    "testdata['q2_vec'] = testdata['q2_cleaned'].apply(sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop NaN values for Train Data\n",
    "traindata = traindata.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as dist\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features Set 3 (FS3)\n",
    "#Word2vec on traindata\n",
    "traindata['euclidean'] = traindata.apply(lambda row: dist.euclidean(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "traindata['manhattan'] = traindata.apply(lambda row: dist.cityblock(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "traindata['canberra'] = traindata.apply(lambda row: dist.canberra(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "traindata['minkowski'] = traindata.apply(lambda row: dist.minkowski(row['q1_vec'], row['q2_vec'], 3) , axis=1)\n",
    "traindata['braycurtis'] = traindata.apply(lambda row: dist.braycurtis(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "traindata['cosine'] = traindata.apply(lambda row: dist.cosine(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "\n",
    "traindata['scew_q1'] = traindata['q1_vec'].apply(stats.skew)\n",
    "traindata['scew_q2'] = traindata['q2_vec'].apply(stats.skew)\n",
    "\n",
    "traindata['kurtosis_q1'] = traindata['q1_vec'].apply(stats.kurtosis)\n",
    "traindata['kurtosis_q2'] = traindata['q2_vec'].apply(stats.kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features Set 3 (FS3)\n",
    "#Word2vec on testdata\n",
    "testdata['euclidean'] = testdata.apply(lambda row: dist.euclidean(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "testdata['manhattan'] = testdata.apply(lambda row: dist.cityblock(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "testdata['canberra'] = testdata.apply(lambda row: dist.canberra(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "testdata['minkowski'] = testdata.apply(lambda row: dist.minkowski(row['q1_vec'], row['q2_vec'], 3) , axis=1)\n",
    "testdata['braycurtis'] = testdata.apply(lambda row: dist.braycurtis(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "testdata['cosine'] = testdata.apply(lambda row: dist.cosine(row['q1_vec'], row['q2_vec']) , axis=1)\n",
    "\n",
    "testdata['scew_q1'] = testdata['q1_vec'].apply(stats.skew)\n",
    "testdata['scew_q2'] = testdata['q2_vec'].apply(stats.skew)\n",
    "\n",
    "testdata['kurtosis_q1'] = testdata['q1_vec'].apply(stats.kurtosis)\n",
    "testdata['kurtosis_q2'] = testdata['q2_vec'].apply(stats.kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check whether the questions pairs have the same start word\n",
    "def same_start_word(row):\n",
    "    if not row['q1_cleaned'] or not row['q2_cleaned']:\n",
    "        return np.nan\n",
    "    return int(row['q1_cleaned'][0] == row['q2_cleaned'][0])\n",
    "\n",
    "#Check for total unique words\n",
    "def total_unique_words(row):\n",
    "    return len(set(row['q1_cleaned']).union(row['q2_cleaned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features Set 4 (FS4)\n",
    "traindata['same_start_word'] = traindata[['q1_cleaned', 'q2_cleaned']].apply(same_start_word, axis=1)\n",
    "traindata['total_unique_words'] = traindata[['q1_cleaned', 'q2_cleaned']].apply(total_unique_words, axis=1)\n",
    "testdata['same_start_word'] = testdata[['q1_cleaned', 'q2_cleaned']].apply(same_start_word, axis=1)\n",
    "testdata['total_unique_words'] = testdata[['q1_cleaned', 'q2_cleaned']].apply(total_unique_words, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
